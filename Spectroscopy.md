## ７　スペクトルの多変量解析

ここでは、スペクトルデータを使って、多変量解析について学習することを目的とします。

<br>

> No matter who pens them, nor in which language they are penned, mere words fail to convey the sadness that befell us on July 17th, 2019, when **Karl Norris** passed away. He became a legend in his own lifetime, long before its end, a legend that will never be surpassed, and created a new world of rapid, chemical-free analysis, the World of Near-infrared Spectroscopy (NIRS). More than 50 years ago his genius recognized that what he had found in the spectra of soybeans could be metamorphosed into a technique that would revolutionize grain analysis. In the decades immediately following, his vision became reality, and the technique has since then expanded far beyond grain analysis into fields too numerous to document. https://icnirs.org/news/a-tribute-to-a-legend/

<br>

### ７−１　スペクトルデータとは

「非破壊検査」、「品質」でネット検索すると青果物の分析についての論文や装置がヒットします。光を用いる分光法、X線などの放射線を用いる方法、または電磁気的な性質を利用する方法などあるなかで、**近赤外分光法**は 1970 年代から農産物の分析に用いられています。なぜ近赤外を使うのでしょう。

私たちが水中の物体をみることができるのは可視光が水を透過するからです。それでも光が達する深さには限界があり、そこからの散乱光を見ることもできません。このことは経験的に皆知るところであり、青が赤よりも透過率が良いために海が青く見えるのも同じ物理現象です。つまり、可視光は水を透過するが、その程度は波長により異なること、赤すなわち波長の長い光はより吸収されるということです。さらに長い赤外線波長を使うとどうでしょう？　赤外線吸収スペクトル法では分子内の官能基の構造を調べることができますが、測定には水を極力排除します。それはOHやNHなどの重要な官能基の吸収付近に覆い被さる吸収を生じることと、ハード的にも光学素子を痛めやすいためです。

近赤外法はちょうど間の波長帯を利用するで、その見えない光は含水物内に浸透して、官能基の情報も与えるという両方の性質を持つと理解できます。具体的には、近赤外分析で用いられる波長は果実の皮を透過して果肉のサンプリングを可能にします。一般には、まず目的の成分が既知のサンプルからスペクトルデータを収集し、次にスペクトルから逆に成分量を予測する**校正モデル**を作成します。このモデルに未知のサンプルから得た近赤外スペクトルを代入して目的成分の予測値を得ます。操作は簡単で測定も迅速で非破壊的であることが近赤外の魅力といえます。

果実のスペクトルは残念ながら持ち合わせていないので、木材の表面から集めたスペクトルで話を進めましょう。４種類の熱帯産材、市場ではメランチと呼ばれる4種のスペクトルです。

<br>

<img src="./img/nir.png" style="zoom:67%;" />

<br>

スペクトルデータは概ね緩やかな曲線で、とてもよく似ているといことに注目してください。ただし、同じ大きさの領域からサンプリングしても有機物の絶対量に応じて吸光の度合が変わるたバックグラウンドは変化します。その影響を除くため２次微分します。下の図は波数8000cm<sup>-1</sup>から4000cm<sup>-1</sup>の領域を取り出して重ね書きしたものですが、絶対値の大きさに影響されずに変化量としたため、異なる資料を比較できるようになりました。このような処理を**正規化**（normalization）といいます。正規化については後で説明します。

<br>

![](./img/2nd_dev.png)

<br>

データが揃うとますますスペクトルは似てきます。しかし若干の差があります。1960年代にKarl Norrisが行なった大豆の研究から始まり、極めて微妙なスペクトルの変化を捉えてサンプルの特徴を取り出す**多変量解析手法**、今でいう**ケモメトリクス**という方法はあらゆる分野で発展し、自動化された品質評価の技術として社会実装されています。

<br>

### ７ー２　次元の圧縮

<br>

さて、スペクトルのデータは、波数8000cm<sup>-1</sup>から4000cm<sup>-1</sup>まで2cm<sup>-1</sup>間隔とすると2000個あります。これらを一つ一つサンプルごとに比べて、クラス分けする手もありますが、ここでは、サンプルを代表するような**特徴量**を取り出すことを考えます。

話は変わりますが、健康診断でよく耳にするBMI (Body Mass Index) という指標は、体重（kg）を身長<sup>2</sup>（m<sup>2</sup>）で除したものです。重さは体積に比例するので、BMIはもう一辺の長さに相当しますから、「あなたの体積にふさわしい重さを超えています」という基準として提案でき、男性ではその値25だと言うわけです。言い換えれば、体重と身長をいう二つのデータ（２次元のデータ）をBMIという一つの指標（１次元）にまとめたものです。このように多次元データの特徴をうまく引き出す幾つかの変数にまとめることを、**次元の圧縮**（Dimensionality Reduction）といいます。

<br>

### ７ー３　主成分分析

<br>

**主成分分析**（Principle Component Analysis）は多次元のデータの情報のもつ傾向を低次元の情報に**縮約**する代表的な方法です。例えば、50サンプルのデータがあり、サンプルごとに６つの測定値があるとします。６次元のデータをグラフ上（直行する６つの軸上）にプロットしてサンプルの特徴をみることを想像できますか。しかし、これを２次元に縮約すればグラフ上に表示でき、データ全体の分布を視覚的にとらえることができるので、データがもつ情報を解釈しやすくなります。

まず縮約から考えてみます。話を簡単にするために変数二つの２次元のデータを定義します。グループAは平均１、標準偏差2の正規分布にしたがう乱数からなる（$$x_{1}, x_{2}$$）のデータ、グループBは平均−２、標準偏差２の正規分布にしたがう乱数からなる（$$x_{1}, x_{2}$$）データです。

<br>

```python
import numpy as np
# np.random.normal(loc,scale,)は、平均loc、標準偏差scaleの正規分布に従う乱数を返します
A=np.random.normal(1,2,(2,10))
B=np.random.normal(-2,2,(2,10))
```

上のスクリプトは実行するたびに新しい乱数が発生しますので、下の表の数字は再現されませんので注意してください。

| A         |           | **B**     |           |
| --------- | --------- | --------- | --------- |
| $$x_{1}$$ | $$x_{2}$$ | $$x_{1}$$ | $$x_{2}$$ |
| 1.34536   | -0.4811   | -2.38     | -2.3792   |
| 1.60766   | 0.66383   | -0.376    | -2.1983   |
| 2.47402   | 2.15362   | -0.7576   | -5.3951   |
| -0.993    | -0.0655   | -1.445    | 0.24701   |
| 4.13782   | 0.09339   | -2.8229   | -3.4589   |
| -2.3125   | 0.47425   | -1.9756   | 1.34693   |
| 2.12943   | 3.14667   | -2.3888   | 0.30033   |
| 0.29949   | -1.2055   | -0.4291   | -1.2403   |
| -0.0528   | 3.08214   | -1.1383   | -1.2218   |
| 3.03482   | 2.889     | -0.9443   | -2.8388   |

<br>

①〜④に$$x_{1}, x_{2}$$を軸として値の分布を示しました。この２次元に分布する点を１次元に縮約するには、①$$x_{1}$$上に正射影する、②$$x_{2}$$上に正射影する、ということも考えられますが、この場合、前者はの$$x_{2}$$の情報を失い、後者はの$$x_{1}$$情報を失うので、全体の傾向をみることができません。③は、それに対して、２次元上のプロットをある軸上に正射影したとき、分散が最も大きくなる軸を見つける方法です。この軸を第一主成分軸$$y_{1}$$といいます。④は第一主成分軸に直行し、２番目に分散が大きくなる軸$$y_{2}$$を見つけます。これを第2主成分軸といいます。

![](./img/PCAs.png)

上の20個のデータで実際の計算をしてみましょう。$$y_{i}$$を次式のように、$$x_{1}$$、$$x_{2}$$に係数$$h_{1j}$$ ,$$h_{2j}$$をかけたものの線形和として表します。
$$
y_{i}=h_{1j}x_{1}+h_{2j}x_{2}
$$
主成分分析では、この$$y_{i}$$の**分散を最大**にする$$h_{ij}$$（$$j=1$$）ベクトルを求めることになります。

を第一主成分軸、二組の対応するデータがどれほどお互いに影響を持ちながら散らばっているかを表します。
この値がそれぞれのデータの持つ分散に近いほどこの二組のデータの相関性は強くなります。（これを相関係数と呼びます。)
共分散が負であれば負の相関（一方の値が増えるともう一方の値が減少する）があり、正であれば正の相関があるといえます。
$$
S_{x_{1}x_{1}}= \frac{1}{n}\sum_{i=0}^{n}(x_{i1}-\bar{x_{1}})^{2}
$$

$$
S_{x_{1}x_{2}} = \frac{1}{n}\sum_{i=0}^{n}(x_{i1}-\bar{x_{1}})(x_{i2}-\bar{x_{2}})
$$

$$
E=\left( \begin{array}{rr}
S_{x_{1}x_{1}} & S_{x_{1}x_{2}} \\ S_{x_{2}x_{1}} & S_{x_{2}x_{2}}
\end{array} \right)
$$









訓練データ$$ xi=(xi1,..,xid)T(i=1,..,N)xi=(xi1,..,xid)T(i=1,..,N)$$の**分散が最大**になる方向を求める。
NN 個のデータからなるデータ行列: X=(x1,..xN)TX=(x1,..xN)T
NN 個の訓練データの平均ベクトル: x¯=(x1¯..,xd¯)Tx¯=(x1¯..,xd¯)T
平均ベクトルを引き算したデータ行列: X¯=(x1−x¯..,xN−x¯)TX¯=(x1−x¯..,xN−x¯)T
とすると、平均化訓練データ行列の分散は

$$ \sum = Var({\bar{X}}) = \frac{1}{N}\bar{X}^{\mathrm{T}}\bar{X}\ $$

で定義される。

単位ベクトル: a=(a1,⋯,ad)Ta=(a1,⋯,ad)T とすると、NN 個のデータ xi−x¯xi−x¯ の単位ベクトル aa への正射影は

$$ s_i = (s_{1}, .. ,s_{d})^{\mathrm{T}} = \bar{Xa} $$

となる。 この変換後のデータの分散は、

$$ Var({{s}}) = \frac{1}{N}{s}^Ts = \frac{1}{N}(\bar{Xa})^{\mathrm{T}}(\bar{Xa}) = \frac{1}{N}{a}^T\bar{X}^{\mathrm{T}}\bar{Xa} = a^TVar({{\bar{X}}})a $$

となる。この分散が最大となる単位ベクトル aa は、係数ベクトルaa のノルムを1となる制約があること利用して、 ラ[グランジ](http://d.hatena.ne.jp/keyword/%A5%B0%A5%E9%A5%F3%A5%B8)ェの未定乗数法を使って求める。

$$ E(a) = a^TVar({{\bar{X}}})a - \lambda(a^T a - 1) $$

を最大にするaaを見つければよい、λλはラ[グランジ](http://d.hatena.ne.jp/keyword/%A5%B0%A5%E9%A5%F3%A5%B8)ェ未定定数である、aaで[微分](http://d.hatena.ne.jp/keyword/%C8%F9%CA%AC)して0としておけば、

$$∂E(a)∂a=2Var(X¯)a−2λa=0∂E(a)∂a=2Var(X¯)a−2λa=0 より Var({X¯})a=λaVar({X¯})a=λa$$

となる。 この式は元のデータの共分散行列に関する**[固有値問題](http://d.hatena.ne.jp/keyword/%B8%C7%CD%AD%C3%CD%CC%E4%C2%EA)**を解くことに等しいので、 分散最大となる単位ベクトル aa は[固有値問題](http://d.hatena.ne.jp/keyword/%B8%C7%CD%AD%C3%CD%CC%E4%C2%EA)を解いて求めた[固有値](http://d.hatena.ne.jp/keyword/%B8%C7%CD%AD%C3%CD)・[固有ベクトル](http://d.hatena.ne.jp/keyword/%B8%C7%CD%AD%A5%D9%A5%AF%A5%C8%A5%EB)の中で、 最大[固有値](http://d.hatena.ne.jp/keyword/%B8%C7%CD%AD%C3%CD)に対応する[固有ベクトル](http://d.hatena.ne.jp/keyword/%B8%C7%CD%AD%A5%D9%A5%AF%A5%C8%A5%EB)を aaとすればよい。

まずは定義式を見てみます。
あるデータの組とがあるとします。
この２つのデータの組の共分散をとすると、



ここで、はそれぞれの要素の平均となっています。

式の中身を見てみると、これはの各々の要素から平均を差し引いたものの掛け算を足し合わせています。
そして最後にこれを値の個数であるで割ることにより共分散を求めています。 
このとき、とが一致していれば、それはの分散を求めていることになります。



ちなみに、ここのをにすると不偏共分散と言われる、ある母集団からサンプリングした場合のサンプリングされた値から推測される共分散を示します。



https://deepage.net/features/numpy-cov.html

まずは線形代数の中における固有値と固有ベクトルの意味についておさらいしておきます。

固有値と固有ベクトルはある正方行列に対して、

https://deepage.net/features/numpy-eigenvalue-vector.html

を満たすようなスカラーとベクトルをそれぞれ 行列に対する固有値と固有ベクトルという表現をします。
このとき は を満たします。

References:

1) A. Savitzky, M. J. E. Golay, Smoothing and Differentiation of Data by Simplified Least Squares Procedures. Analytical Chemistry, 1964, 36 (8), pp 1627-1639.
2) Numerical Recipes 3rd Edition: The Art of Scientific Computing W.H. Press, S.A. Teukolsky, W.T. Vetterling, B.P. Flannery Cambridge University Press ISBN-13: 9780521880688
